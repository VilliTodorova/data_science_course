{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecac9c20-e9eb-4b83-b9b3-edd1ec32d57a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your imports here\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import re\n",
    "\n",
    "import os\n",
    "from PIL import Image\n",
    "import imagehash\n",
    "\n",
    "from tensorflow.keras.applications.resnet50 import ResNet50, preprocess_input\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import glob\n",
    "\n",
    "import tensorflow as tf\n",
    "import cv2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f409693-729b-4a8a-b69e-767deb80b648",
   "metadata": {},
   "source": [
    "# Working with Images Lab\n",
    "## Information retrieval, preprocessing, and feature extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71a328e6-f43f-40c3-abf3-cb650ff59141",
   "metadata": {},
   "source": [
    "In this lab, you'll work with images of felines (cats), which have been classified according to their taxonomy. Each subfolder contains images of a particular species. The dataset is located [here](https://www.kaggle.com/datasets/datahmifitb/felis-taxonomy-image-classification) but it's also provided to you in the `data/` folder."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d41915a7",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2930cdea-105b-4f27-b28f-30323036b6c1",
   "metadata": {},
   "source": [
    "### Problem 1. Some exploration (1 point)\n",
    "How many types of cats are there? How many images do we have of each? What is a typical image size? Are there any outliers in size?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "804efeb0",
   "metadata": {},
   "source": [
    "First, we are going to read the data and store the needed information in variables for species, number and size of the images. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72d6fbdb-37b4-4ce4-a3fa-65954c377539",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_folder = 'data'\n",
    "\n",
    "species = []\n",
    "num_images = []\n",
    "image_sizes = []\n",
    "\n",
    "for subfolder in os.listdir(data_folder):\n",
    "    subfolder_path = os.path.join(data_folder, subfolder)\n",
    "    if os.path.isdir(subfolder_path):\n",
    "        # List images in the subfolder\n",
    "        images = [img for img in os.listdir(subfolder_path) if img.lower().endswith(('png', 'jpg', 'jpeg', 'tiff', 'bmp', 'gif'))]\n",
    "        \n",
    "        # Store species and number of images\n",
    "        species.append(subfolder)\n",
    "        num_images.append(len(images))\n",
    "        \n",
    "        # Analyze image sizes\n",
    "        for image_name in images:\n",
    "            image_path = os.path.join(subfolder_path, image_name)\n",
    "            try:\n",
    "                with Image.open(image_path) as img:\n",
    "                    image_sizes.append(img.size)\n",
    "            except Exception as e:\n",
    "                print(f\"Error opening image {image_path}: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42f007e3",
   "metadata": {},
   "source": [
    "Second, we are going to create a dataframe with image height and width and then calculate the typical image size using mean and median. In order to analyze the size and detect any potential outliers, we are going to use the $IQR$ method, which focuses on the middle 50% of the data. The Interquartile Range is the range between the first quartile ($Q1$) and the third quartile ($Q3$) by the formula $IQR = Q3 - Q1$. The outliers are typically defined as data points which fall below $Q1 - 1.5 * IQR$ or above $Q3 + 1.5 * IQR$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c15a21a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sizes_df = pd.DataFrame(image_sizes, columns=['Width', 'Height'])\n",
    "\n",
    "# Calculate typical image size\n",
    "mean_size = sizes_df.mean()\n",
    "median_size = sizes_df.median()\n",
    "\n",
    "# Detect outliers using IQR method\n",
    "Q1 = sizes_df.quantile(0.25)\n",
    "Q3 = sizes_df.quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "outliers = sizes_df[(sizes_df < (Q1 - 1.5 * IQR)) | (sizes_df > (Q3 + 1.5 * IQR))]\n",
    "\n",
    "# Output the results\n",
    "species_images_count = dict(zip(species, num_images))\n",
    "typical_size = (mean_size, median_size)\n",
    "outliers_count = outliers.dropna().shape[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0d4eccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "for k, v in species_images_count.items():\n",
    "    print(f'{k}: {v}')\n",
    "\n",
    "print(f'Mean image size: Width = {mean_size[\"Width\"]:.2f}, Height = {mean_size[\"Height\"]:.2f}')\n",
    "print(f'Median image size: Width = {median_size[\"Width\"]:.1f}, Height = {median_size[\"Height\"]:.1f}')\n",
    "\n",
    "print(f'Number of outliers: {outliers_count}.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a54a788-a46a-4683-b78e-6e9c9e25c46d",
   "metadata": {},
   "source": [
    "### Problem 2. Duplicat(e)s (1 point)\n",
    "Find a way to filter out (remove) identical images. I would recommnend using file hashes, but there are many approaches. Keep in mind that during file saving, recompression, etc., a lot of artifacts can change the file content (bytes), but not visually."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c676d2ca",
   "metadata": {},
   "source": [
    "We are going to need a set to store the unique image hashes and traverse the data folder in order to go through all of the images to check for duplicates, which in fact is quite straight-forward using hashes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82a8920c-1237-488e-8267-c1f6cadbdb38",
   "metadata": {},
   "outputs": [],
   "source": [
    "species = []\n",
    "num_images = []\n",
    "image_hashes = set()\n",
    "removed_images = 0\n",
    "\n",
    "# Traverse the data folder\n",
    "for subfolder in os.listdir(data_folder):\n",
    "    subfolder_path = os.path.join(data_folder, subfolder)\n",
    "    if os.path.isdir(subfolder_path):\n",
    "        images = [img for img in os.listdir(subfolder_path) if img.lower().endswith(('png', 'jpg', 'jpeg', 'tiff', 'bmp', 'gif'))]\n",
    "        \n",
    "        species.append(subfolder)\n",
    "        unique_images = 0\n",
    "        \n",
    "        # Analyze image hashes\n",
    "        for image_name in images:\n",
    "            image_path = os.path.join(subfolder_path, image_name)\n",
    "            try:\n",
    "                with Image.open(image_path) as img:\n",
    "                    img_hash = imagehash.average_hash(img)\n",
    "                    if img_hash not in image_hashes:\n",
    "                        image_hashes.add(img_hash)\n",
    "                        unique_images += 1\n",
    "                    else:\n",
    "                        removed_images += 1\n",
    "            except Exception as e:\n",
    "                print(f\"Error opening image {image_path}: {e}\")\n",
    "        \n",
    "        num_images.append(unique_images)\n",
    "\n",
    "species_images_count = dict(zip(species, num_images))\n",
    "\n",
    "for specie, count in species_images_count.items():\n",
    "    print(f'Species: {specie}, Number of unique images: {count}')\n",
    "\n",
    "print(f'Number of removed images: {removed_images}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "918359ba-8d97-42c4-82ec-975931ec7fb9",
   "metadata": {},
   "source": [
    "### Problem 3. Loading a model (2 points)\n",
    "Find a suitable, trained convolutional neural network classifier. I recommend `ResNet50` as it's small enough to run well on any machine and powerful enough to make reasonable predictions. Most ready-made classifiers have been trained for 1000 classes.\n",
    "\n",
    "You'll need to install libraries and possibly tinker with configurations for this task. When you're done, display the total number of layers and the total number of parameters. For ResNet50, you should expect around 50 layers and 25M parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90bedfcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ResNet50(weights='imagenet')\n",
    "\n",
    "model.summary()\n",
    "\n",
    "total_layers = len(model.layers)\n",
    "total_params = model.count_params()\n",
    "\n",
    "print(f'Total number of layers: {total_layers}')\n",
    "print(f'Total number of parameters: {total_params:,}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fef6b32-db23-4616-b497-e78a41a1d487",
   "metadata": {},
   "source": [
    "### Problem 4. Prepare the images (1 point)\n",
    "You'll need to prepare the images for passing to the model. To do so, they have to be resized to the same dimensions. Most available models have a specific requirement for sizes. You may need to do additional preprocessing, depending on the model requirements. These requirements should be easily available in the model documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87300cdc-b8b0-4fae-83dc-b0b15639ec71",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ResNet50(weights='imagenet')\n",
    "\n",
    "def preprocess_image(img_path):\n",
    "    img = image.load_img(img_path, target_size=(224, 224))\n",
    "    img_array = image.img_to_array(img)\n",
    "    img_array = np.expand_dims(img_array, axis=0)\n",
    "    img_array = preprocess_input(img_array)\n",
    "    return img_array\n",
    "\n",
    "def preprocess_images_in_directory(directory_path):\n",
    "    img_paths = glob.glob(os.path.join(directory_path, '**', '*.jpg'), recursive=True)\n",
    "    preprocessed_images = []\n",
    "\n",
    "    if not img_paths:\n",
    "        print(\"No images found in the directory.\")\n",
    "        return np.array([])\n",
    "\n",
    "    for img_path in img_paths:\n",
    "        preprocessed_img = preprocess_image(img_path)\n",
    "        if preprocessed_img is not None:\n",
    "            preprocessed_images.append(preprocessed_img)\n",
    "\n",
    "    if preprocessed_images:\n",
    "        batch = np.vstack(preprocessed_images)\n",
    "        return batch\n",
    "    else:\n",
    "        print(\"No valid images to preprocess.\")\n",
    "        return np.array([])\n",
    "\n",
    "directory_path = 'data'\n",
    "preprocessed_batch = preprocess_images_in_directory(directory_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6ade4b3-62ac-4f4e-aa02-3c65dc9ab3d9",
   "metadata": {},
   "source": [
    "### Problem 5. Load the images efficiently (1 point)\n",
    "Now that you've seen how to prepare the images for passing to the model... find a way to do it efficiently. Instead of loading the entire dataset in the RAM, read the images in batches (e.g. 4 images at a time). The goal is to read these, preprocess them, maybe save the preprocessed results in RAM.\n",
    "\n",
    "If you've already done this in one of the previous problems, just skip this one. You'll get your point for it.\n",
    "\n",
    "\\* Even better, save the preprocessed image arrays (they will not be valid .jpg file) as separate files, so you can load them \"lazily\" in the following steps. This is a very common optimization to work with large datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0afb4c7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_image(img_path):\n",
    "    img = image.load_img(img_path, target_size=(224, 224))\n",
    "    img_array = image.img_to_array(img)\n",
    "    img_array = preprocess_input(img_array)\n",
    "    return img_array\n",
    "\n",
    "def preprocess_images_in_batches(directory_path, batch_size=4, save_dir='preprocessed_batches'):\n",
    "    img_paths = glob.glob(os.path.join(directory_path, '**', '*.jpg'), recursive=True) + \\\n",
    "                glob.glob(os.path.join(directory_path, '**', '*.png'), recursive=True)\n",
    "\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "    for i in range(0, len(img_paths), batch_size):\n",
    "        batch_paths = img_paths[i:i + batch_size]\n",
    "        batch = np.array([preprocess_image(p) for p in batch_paths])\n",
    "        batch_index = i // batch_size\n",
    "        np.save(os.path.join(save_dir, f'batch_{batch_index}.npy'), batch)\n",
    "\n",
    "    print(f\"Processed and saved {len(img_paths) // batch_size + 1} batches.\")\n",
    "\n",
    "# Call this function to preprocess and save images in batches\n",
    "preprocess_images_in_batches('data')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a63ddc6a",
   "metadata": {},
   "source": [
    "This is a good place to fine-tune our model in order to make better predictions based on our dataset and to serve us better for the further problems in this lab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36dd6085",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load ResNet50 with ImageNet weights, excluding the top layers\n",
    "base_model = ResNet50(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "\n",
    "# Freeze the layers except the last few\n",
    "for layer in base_model.layers[:-4]:\n",
    "    layer.trainable = False\n",
    "\n",
    "# Add custom classification layers\n",
    "x = base_model.output\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "x = Dropout(0.5)(x)  # Add dropout for regularization\n",
    "x = Dense(1024, activation='relu')(x)\n",
    "predictions = Dense(len(class_names), activation='softmax')(x)\n",
    "\n",
    "# Combine the base model and the custom layers into a new model\n",
    "model = Model(inputs=base_model.input, outputs=predictions)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=Adam(learning_rate=0.0001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Convert labels to categorical\n",
    "labels_categorical = tf.keras.utils.to_categorical(true_labels, num_classes=len(class_names))\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(preprocessed_batch, labels_categorical, epochs=10, validation_split=0.2)\n",
    "\n",
    "# Optionally, unfreeze some more layers and fine-tune\n",
    "for layer in base_model.layers[-10:]:  # Unfreeze the last 10 layers\n",
    "    layer.trainable = True\n",
    "\n",
    "# Recompile the model with a lower learning rate\n",
    "model.compile(optimizer=Adam(learning_rate=0.0001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Continue training the model with the unfrozen layers\n",
    "history_fine_tune = model.fit(preprocessed_batch, labels_categorical, epochs=10, validation_split=0.2)\n",
    "\n",
    "# Save the fine-tuned model for future use\n",
    "model.save('fine_tuned_resnet50.keras')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f62f557e-fae0-42f1-aa3b-4402a9be05d2",
   "metadata": {},
   "source": [
    "### Problem 6. Predictions (1 point)\n",
    "Finally, you're ready to get into the meat of the problem. Obtain predictions from your model and evaluate them. This will likely involve manual work to decide how the returned classes relate to the original ones.\n",
    "\n",
    "Create a [confusion matrix](https://en.wikipedia.org/wiki/Confusion_matrix) to evaluate the classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72923e55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to clip label values within the valid range\n",
    "def clip_labels(labels, num_classes):\n",
    "    return np.clip(labels, 0, num_classes - 1)\n",
    "\n",
    "# Function to load preprocessed batch\n",
    "def load_preprocessed_batch(batch_index, save_dir='preprocessed_batches'):\n",
    "    batch_path = os.path.join(save_dir, f'batch_{batch_index}.npy')\n",
    "    return np.load(batch_path)\n",
    "\n",
    "# Function to make predictions\n",
    "def make_predictions(num_batches, model, save_dir='preprocessed_batches'):\n",
    "    all_predictions = []\n",
    "    for batch_index in range(num_batches):\n",
    "        batch = load_preprocessed_batch(batch_index, save_dir)\n",
    "        predictions = model.predict(batch)\n",
    "        all_predictions.extend(predictions)\n",
    "    return np.argmax(np.array(all_predictions), axis=1)\n",
    "\n",
    "# Function to plot confusion matrix\n",
    "def plot_confusion_matrix(true_labels, predicted_labels, num_classes):\n",
    "    # Ensure the labels are within the valid range\n",
    "    true_labels = clip_labels(true_labels, num_classes)\n",
    "    predicted_labels = clip_labels(predicted_labels, num_classes)\n",
    "\n",
    "    # Print the maximum and minimum values of labels for debugging\n",
    "    print(f\"True labels range: {true_labels.min()} to {true_labels.max()}\")\n",
    "    print(f\"Predicted labels range: {predicted_labels.min()} to {predicted_labels.max()}\")\n",
    "\n",
    "    # Create confusion matrix\n",
    "    cm = tf.math.confusion_matrix(true_labels, predicted_labels, num_classes=num_classes)\n",
    "    cm_np = cm.numpy()\n",
    "    \n",
    "    # Plot confusion matrix\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    plt.imshow(cm_np, cmap=plt.cm.Blues)\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.ylabel('True Label')\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(num_classes)\n",
    "    plt.xticks(tick_marks)\n",
    "    plt.yticks(tick_marks)\n",
    "\n",
    "    for i in range(num_classes):\n",
    "        for j in range(num_classes):\n",
    "            plt.text(j, i, int(cm_np[i, j]), ha='center', va='center', \n",
    "                     color='white' if cm_np[i, j] > cm_np.max() / 2 else 'black')\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "def get_true_labels(directory_path, class_to_label):\n",
    "    true_labels = []\n",
    "    img_paths = glob.glob(os.path.join(directory_path, '**', '*.jpg'), recursive=True) + \\\n",
    "                glob.glob(os.path.join(directory_path, '**', '*.png'), recursive=True)\n",
    "\n",
    "    for img_path in img_paths:\n",
    "        class_name = os.path.basename(os.path.dirname(img_path))\n",
    "        label = class_to_label.get(class_name)\n",
    "        if label is not None:\n",
    "            true_labels.append(label)\n",
    "        else:\n",
    "            print(f\"Warning: Class name '{class_name}' not found in class_to_label mapping.\")\n",
    "    return true_labels\n",
    "\n",
    "# Directory path and class names\n",
    "directory_path = 'data'  # The path to your dataset\n",
    "class_names = sorted([d for d in os.listdir(directory_path) if os.path.isdir(os.path.join(directory_path, d))])\n",
    "class_to_label = {class_name: idx for idx, class_name in enumerate(class_names)}\n",
    "\n",
    "# Get true labels\n",
    "true_labels = get_true_labels(directory_path, class_to_label)\n",
    "\n",
    "# Make predictions\n",
    "num_batches = len(os.listdir('preprocessed_batches'))\n",
    "predicted_labels = make_predictions(num_batches, model)\n",
    "\n",
    "# Plot confusion matrix\n",
    "plot_confusion_matrix(true_labels, predicted_labels, num_classes=len(class_names))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5020f81e-721f-4882-83eb-80379f7a20ac",
   "metadata": {},
   "source": [
    "### Problem 7. Grayscale (1 point)\n",
    "Converting the images to grayscale should affect the classification negatively, as we lose some of the color information.\n",
    "\n",
    "Find a way to preprocess the images to grayscale (using what you already have in Problem 4 and 5), pass them to the model, and compare the classification results to the previous ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af0a53f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_image_grayscale(img_path):\n",
    "    img = image.load_img(img_path, target_size=(224, 224))\n",
    "    \n",
    "    img_array = image.img_to_array(img)\n",
    "    \n",
    "    # Convert to grayscale\n",
    "    img_gray = cv2.cvtColor(img_array.astype('uint8'), cv2.COLOR_RGB2GRAY)\n",
    "    \n",
    "    # Expand dimensions to match the input shape of the model\n",
    "    img_gray = np.expand_dims(img_gray, axis=-1)\n",
    "    img_gray = np.repeat(img_gray, 3, axis=-1)\n",
    "\n",
    "    img_gray = preprocess_input(img_gray)\n",
    "    \n",
    "    return img_gray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "895bffd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_images_in_batches_grayscale(directory_path, batch_size=4, save_dir='preprocessed_batches_grayscale'):\n",
    "    img_paths = glob.glob(os.path.join(directory_path, '**', '*.jpg'), recursive=True) + \\\n",
    "                glob.glob(os.path.join(directory_path, '**', '*.png'), recursive=True)\n",
    "\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "    for i in range(0, len(img_paths), batch_size):\n",
    "        batch_paths = img_paths[i:i + batch_size]\n",
    "        batch = np.array([preprocess_image_grayscale(p) for p in batch_paths])\n",
    "        batch_index = i // batch_size\n",
    "        np.save(os.path.join(save_dir, f'batch_{batch_index}.npy'), batch)\n",
    "\n",
    "    print(f\"Processed and saved {len(img_paths) // batch_size + 1} grayscale batches.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0608b7dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load preprocessed batch for grayscale\n",
    "def load_preprocessed_batch_grayscale(batch_index, save_dir='preprocessed_batches_grayscale'):\n",
    "    batch_path = os.path.join(save_dir, f'batch_{batch_index}.npy')\n",
    "    return np.load(batch_path)\n",
    "\n",
    "# Make predictions for grayscale images\n",
    "def make_predictions_grayscale(num_batches, model, save_dir='preprocessed_batches_grayscale'):\n",
    "    all_predictions = []\n",
    "    for batch_index in range(num_batches):\n",
    "        batch = load_preprocessed_batch_grayscale(batch_index, save_dir)\n",
    "        predictions = model.predict(batch)\n",
    "        all_predictions.extend(predictions)\n",
    "    return np.argmax(np.array(all_predictions), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f8eb359",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to clip label values within the valid range\n",
    "def clip_labels(labels, num_classes):\n",
    "    return np.clip(labels, 0, num_classes - 1)\n",
    "\n",
    "# Plot confusion matrix\n",
    "def plot_confusion_matrix(true_labels, predicted_labels, num_classes):\n",
    "    true_labels = clip_labels(true_labels, num_classes)\n",
    "    predicted_labels = clip_labels(predicted_labels, num_classes)\n",
    "\n",
    "    print(f\"True labels range: {true_labels.min()} to {true_labels.max()}\")\n",
    "    print(f\"Predicted labels range: {predicted_labels.min()} to {predicted_labels.max()}\")\n",
    "\n",
    "    cm = tf.math.confusion_matrix(true_labels, predicted_labels, num_classes=num_classes)\n",
    "    cm_np = cm.numpy()\n",
    "\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    plt.imshow(cm_np, cmap=plt.cm.Blues)\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.ylabel('True Label')\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(num_classes)\n",
    "    plt.xticks(tick_marks, class_names, rotation=45)\n",
    "    plt.yticks(tick_marks, class_names)\n",
    "\n",
    "    for i in range(num_classes):\n",
    "        for j in range(num_classes):\n",
    "            plt.text(j, i, int(cm_np[i, j]), ha='center', va='center', \n",
    "                     color='white' if cm_np[i, j] > cm_np.max() / 2 else 'black')\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "# Function to get true labels\n",
    "def get_true_labels(directory_path, class_to_label):\n",
    "    true_labels = []\n",
    "    img_paths = glob.glob(os.path.join(directory_path, '**', '*.jpg'), recursive=True) + \\\n",
    "                glob.glob(os.path.join(directory_path, '**', '*.png'), recursive=True)\n",
    "\n",
    "    for img_path in img_paths:\n",
    "        class_name = os.path.basename(os.path.dirname(img_path))\n",
    "        label = class_to_label.get(class_name)\n",
    "        if label is not None:\n",
    "            true_labels.append(label)\n",
    "        else:\n",
    "            print(f\"Warning: Class name '{class_name}' not found in class_to_label mapping.\")\n",
    "    return true_labels\n",
    "\n",
    "# Directory path and class names\n",
    "directory_path = 'data'\n",
    "class_names = sorted([d for d in os.listdir(directory_path) if os.path.isdir(os.path.join(directory_path, d))])\n",
    "class_to_label = {class_name: idx for idx, class_name in enumerate(class_names)}\n",
    "\n",
    "# Get true labels\n",
    "true_labels = get_true_labels(directory_path, class_to_label)\n",
    "\n",
    "# Preprocess and predict for RGB images\n",
    "preprocess_images_in_batches(directory_path, batch_size=4, save_dir='preprocessed_batches')\n",
    "num_batches_rgb = len(os.listdir('preprocessed_batches'))\n",
    "predicted_labels_rgb = make_predictions(num_batches_rgb, model, save_dir='preprocessed_batches')\n",
    "print(\"Evaluating RGB predictions...\")\n",
    "plot_confusion_matrix(true_labels, predicted_labels_rgb, num_classes=len(class_names))\n",
    "\n",
    "# Preprocess and predict for grayscale images\n",
    "preprocess_images_in_batches_grayscale(directory_path, batch_size=4, save_dir='preprocessed_batches_grayscale')\n",
    "num_batches_gray = len(os.listdir('preprocessed_batches_grayscale'))\n",
    "predicted_labels_gray = make_predictions_grayscale(num_batches_gray, model, save_dir='preprocessed_batches_grayscale')\n",
    "print(\"Evaluating Grayscale predictions...\")\n",
    "plot_confusion_matrix(true_labels, predicted_labels_gray, num_classes=len(class_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e585e663-2f06-4562-8bea-504b3d583c66",
   "metadata": {},
   "source": [
    "### Problem 8. Deep image features (1 point)\n",
    "Find a way to extract one-dimensional vectors (features) for each (non-grayscale) image, using your model. This is typically done by \"short-circuiting\" the model output to be an intermediate layer, while keeping the input the same. \n",
    "\n",
    "In case the outputs (also called feature maps) have different shapes, you can flatten them in different ways. Try to not create huge vectors; the goal is to have a relatively short sequence of numbers which describes each image.\n",
    "\n",
    "You may find a tutorial like [this](https://towardsdatascience.com/exploring-feature-extraction-with-cnns-345125cefc9a) pretty useful but note your implementation will depend on what model (and framework) you've decided to use.\n",
    "\n",
    "It's a good idea to save these as one or more files, so you'll spare yourself a ton of preprocessing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53ab8469",
   "metadata": {},
   "source": [
    "Let's work on modifying the model first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71347edf-a13b-4683-beb2-af0a093a0cd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_extractor = Model(inputs=model.input, outputs=model.get_layer('global_average_pooling2d_4').output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87f1a756",
   "metadata": {},
   "source": [
    "Next we are going to extract the features and save them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5daeb194",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(img_batch, feature_extractor):\n",
    "    features = feature_extractor.predict(img_batch)\n",
    "    return features\n",
    "\n",
    "def get_image_paths(directory_path):\n",
    "    img_paths = glob.glob(os.path.join(directory_path, '**', '*.jpg'), recursive=True) + \\\n",
    "                glob.glob(os.path.join(directory_path, '**', '*.png'), recursive=True)\n",
    "    return img_paths\n",
    "\n",
    "def save_features(directory_path, feature_extractor, batch_size=4, save_dir='extracted_features'):\n",
    "    img_paths = get_image_paths(directory_path)\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "    all_features = []\n",
    "    for i in range(0, len(img_paths), batch_size):\n",
    "        batch_paths = img_paths[i:i + batch_size]\n",
    "        batch = np.array([preprocess_image(p) for p in batch_paths])\n",
    "        features = extract_features(batch, feature_extractor)\n",
    "        all_features.append(features)\n",
    "\n",
    "    all_features = np.vstack(all_features)\n",
    "    np.save(os.path.join(save_dir, 'extracted_features.npy'), all_features)\n",
    "\n",
    "    # Save image paths for reference\n",
    "    with open(os.path.join(save_dir, 'img_paths.txt'), 'w') as f:\n",
    "        for path in img_paths:\n",
    "            f.write(path + '\\n')\n",
    "\n",
    "    print(f\"Extracted and saved features for {len(img_paths)} images.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d25f795e",
   "metadata": {},
   "outputs": [],
   "source": [
    "directory_path = 'data'\n",
    "\n",
    "save_features(directory_path, feature_extractor, batch_size=4, save_dir='extracted_features')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9bd0ab0-910a-4dad-9383-011e7d7616e1",
   "metadata": {},
   "source": [
    "### Problem 9. Putting deep image features to use (1 points)\n",
    "Try to find similar images, using a similarity metric on the features you got in the previous problem. Two good metrics are `mean squared error` and `cosine similarity`. How do they work? Can you spot images that look too similar? Can you explain why?\n",
    "\n",
    "\\* If we were to take Fourier features (in a similar manner, these should be a vector of about the same length), how do they compare to the deep features; i.e., which features are better to \"catch\" similar images?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca980f2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate MSE between two vectors\n",
    "def mse(x, y):\n",
    "    return np.mean((x - y) ** 2)\n",
    "\n",
    "# Function to calculate cosine similarity between two vectors\n",
    "def cosine_sim(x, y):\n",
    "    dot_product = np.dot(x, y)\n",
    "    norm_x = np.linalg.norm(x)\n",
    "    norm_y = np.linalg.norm(y)\n",
    "    return dot_product / (norm_x * norm_y)\n",
    "\n",
    "# Function to find most similar images based on features\n",
    "def find_similar_images(features, metric='cosine'):\n",
    "    num_images = features.shape[0]\n",
    "    similarity_matrix = np.zeros((num_images, num_images))\n",
    "    \n",
    "    for i in range(num_images):\n",
    "        for j in range(num_images):\n",
    "            if metric == 'mse':\n",
    "                similarity_matrix[i, j] = mse(features[i], features[j])\n",
    "            elif metric == 'cosine':\n",
    "                similarity_matrix[i, j] = cosine_sim(features[i], features[j])\n",
    "\n",
    "    most_similar_pairs = np.argsort(-similarity_matrix, axis=1) if metric == 'cosine' else np.argsort(similarity_matrix, axis=1)\n",
    "    return similarity_matrix, most_similar_pairs\n",
    "\n",
    "# Load features from directory\n",
    "def load_features_from_directory(directory_path):\n",
    "    npy_files = [os.path.join(directory_path, f) for f in os.listdir(directory_path) if f.endswith('.npy')]\n",
    "    feature_list = [np.load(file) for file in npy_files]\n",
    "    return np.concatenate(feature_list, axis=0)\n",
    "\n",
    "# Plot similarity matrix\n",
    "def plot_similarity_matrix(similarity_matrix, title='Similarity Matrix'):\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(similarity_matrix, cmap='viridis')\n",
    "    plt.title(title)\n",
    "    plt.xlabel('Image Index')\n",
    "    plt.ylabel('Image Index')\n",
    "    plt.show()\n",
    "\n",
    "# Load image paths\n",
    "def load_img_paths(file_path):\n",
    "    with open(file_path, 'r') as f:\n",
    "        return [line.strip() for line in f]\n",
    "\n",
    "# Display top N similar images in a grid\n",
    "def display_similar_images(img_paths, most_similar_pairs, top_n=5):\n",
    "    num_images = len(img_paths)\n",
    "    \n",
    "    for i in range(num_images):\n",
    "        plt.figure(figsize=(20, 5))\n",
    "        \n",
    "        # Display the original image\n",
    "        plt.subplot(1, top_n + 1, 1)\n",
    "        img = plt.imread(img_paths[i])\n",
    "        plt.imshow(img)\n",
    "        plt.title(f'Original Image {i}')\n",
    "        plt.axis('off')\n",
    "\n",
    "        # Display similar images\n",
    "        unique_similar_indices = set()\n",
    "        for j in range(1, top_n + 1):\n",
    "            sim_idx = most_similar_pairs[i, j]\n",
    "            if 0 <= sim_idx < num_images and sim_idx != i:\n",
    "                if sim_idx not in unique_similar_indices:\n",
    "                    unique_similar_indices.add(sim_idx)\n",
    "                    sim_img = plt.imread(img_paths[sim_idx])\n",
    "                    plt.subplot(1, top_n + 1, j + 1)\n",
    "                    plt.imshow(sim_img)\n",
    "                    plt.title(f'Similar {j} (Idx {sim_idx})')\n",
    "                    plt.axis('off')\n",
    "                    \n",
    "                # Stop if we have displayed the required number of unique similar images\n",
    "                if len(unique_similar_indices) >= top_n:\n",
    "                    break\n",
    "        \n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8ad136f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the features and image paths\n",
    "features = load_features_from_directory('extracted_features')\n",
    "img_paths = load_img_paths('extracted_features/img_paths.txt')\n",
    "print(features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8503267f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_features_from_directory(directory_path):\n",
    "    npy_files = [os.path.join(directory_path, f) for f in os.listdir(directory_path) if f.endswith('.npy')]\n",
    "    print(f\"Found {len(npy_files)} .npy files.\")\n",
    "    feature_list = []\n",
    "    for file in npy_files:\n",
    "        data = np.load(file)\n",
    "        print(f\"{file} shape: {data.shape}\")\n",
    "        feature_list.append(data)\n",
    "    features = np.concatenate(feature_list, axis=0)\n",
    "    print(f\"Concatenated features shape: {features.shape}\")\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2c2e06c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate similarity matrices and find most similar pairs\n",
    "similarity_matrix_cosine, most_similar_pairs_cosine = find_similar_images(features, metric='cosine')\n",
    "similarity_matrix_mse, most_similar_pairs_mse = find_similar_images(features, metric='mse')\n",
    "\n",
    "# Plot similarity matrices\n",
    "plot_similarity_matrix(similarity_matrix_cosine, title='Cosine Similarity Matrix')\n",
    "plot_similarity_matrix(similarity_matrix_mse, title='MSE Similarity Matrix')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a85200b",
   "metadata": {},
   "source": [
    "CAUTION BEFORE EXECUTING THE NEXT CODE CELL!\n",
    "\n",
    "For some reason I see dublicates on each image and on top of that I cannot seem to make the code work to display only the 5 most similar pairs and break out of the loop, thus the following cell leads to an infinite loop. I tried though."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2cbff7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display similar images\n",
    "display_similar_images(img_paths, most_similar_pairs_cosine, top_n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5276ec7b-7736-4508-99bd-29516464e3f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate MSE between two vectors\n",
    "def mse(x, y):\n",
    "    return np.mean((x - y) ** 2)\n",
    "\n",
    "# Function to calculate cosine similarity between two vectors\n",
    "def cosine_sim(x, y):\n",
    "    dot_product = np.dot(x, y)\n",
    "    norm_x = np.linalg.norm(x)\n",
    "    norm_y = np.linalg.norm(y)\n",
    "    return dot_product / (norm_x * norm_y)\n",
    "\n",
    "# Function to find most similar images based on features\n",
    "def find_similar_images(features, metric='cosine'):\n",
    "    num_images = features.shape[0]\n",
    "    similarity_matrix = np.zeros((num_images, num_images))\n",
    "\n",
    "    for i in range(num_images):\n",
    "        for j in range(num_images):\n",
    "            if metric == 'mse':\n",
    "                similarity_matrix[i, j] = mse(features[i], features[j])\n",
    "            elif metric == 'cosine':\n",
    "                similarity_matrix[i, j] = cosine_sim(features[i], features[j])\n",
    "\n",
    "    if metric == 'cosine':\n",
    "        most_similar_pairs = np.argsort(-similarity_matrix, axis=1)\n",
    "    else:\n",
    "        most_similar_pairs = np.argsort(similarity_matrix, axis=1)\n",
    "\n",
    "    return similarity_matrix, most_similar_pairs\n",
    "\n",
    "\n",
    "# Load all .npy files from the directory\n",
    "def load_features_from_directory(directory_path):\n",
    "    feature_list = []\n",
    "    npy_files = [f for f in os.listdir(directory_path) if f.endswith('.npy')]\n",
    "    \n",
    "    for npy_file in npy_files:\n",
    "        file_path = os.path.join(directory_path, npy_file)\n",
    "        features = np.load(file_path)\n",
    "        feature_list.append(features)\n",
    "    \n",
    "    # Combine all the features into a single numpy array\n",
    "    all_features = np.concatenate(feature_list, axis=0)\n",
    "    return all_features\n",
    "\n",
    "# Example usage with extracted features\n",
    "directory_path = 'extracted_features'  # Directory containing .npy files\n",
    "features = load_features_from_directory(directory_path)  # Load precomputed features\n",
    "\n",
    "# Calculate similarity matrices and find most similar pairs\n",
    "similarity_matrix_cosine, most_similar_pairs_cosine = find_similar_images(features, metric='cosine')\n",
    "similarity_matrix_mse, most_similar_pairs_mse = find_similar_images(features, metric='mse')\n",
    "\n",
    "# Optional: print out the most similar pairs\n",
    "print(\"Most similar pairs (Cosine Similarity):\")\n",
    "print(most_similar_pairs_cosine)\n",
    "\n",
    "print(\"Most similar pairs (MSE):\")\n",
    "print(most_similar_pairs_mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "275a7788",
   "metadata": {},
   "source": [
    "We can use heatmaps to better understand these similarity pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe0091a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_similarity_matrix(similarity_matrix, title='Similarity Matrix'):\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(similarity_matrix, cmap='viridis')\n",
    "    plt.title(title)\n",
    "    plt.xlabel('Image Index')\n",
    "    plt.ylabel('Image Index')\n",
    "    plt.show()\n",
    "\n",
    "# Plotting Cosine Similarity Matrix\n",
    "plot_similarity_matrix(similarity_matrix_cosine, title='Cosine Similarity Matrix')\n",
    "\n",
    "# Plotting MSE Similarity Matrix\n",
    "plot_similarity_matrix(similarity_matrix_mse, title='MSE Similarity Matrix')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c551e3da",
   "metadata": {},
   "source": [
    "Or even display the most similar images in a grid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86ab6d63",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_img_paths(file_path):\n",
    "    with open(file_path, 'r') as f:\n",
    "        img_paths = [line.strip() for line in f]\n",
    "    return img_paths\n",
    "\n",
    "# Load the features and image paths\n",
    "features = np.load('extracted_features/extracted_features.npy')\n",
    "img_paths = load_img_paths('extracted_features/img_paths.txt')\n",
    "\n",
    "# Example usage for visualization\n",
    "print(f\"Loaded {len(img_paths)} image paths and features.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e9db62d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_similar_images(img_paths, most_similar_pairs, top_n=5):\n",
    "    num_images = len(img_paths)\n",
    "    break_counter = 0\n",
    "\n",
    "    for i in range(num_images):\n",
    "        if break_counter > 5:\n",
    "            break\n",
    "        plt.figure(figsize=(20, 5))\n",
    "\n",
    "        # Display the original image\n",
    "        try:\n",
    "            img = plt.imread(img_paths[i])\n",
    "            plt.subplot(1, top_n + 1, 1)\n",
    "            plt.imshow(img)\n",
    "            plt.title(f'Original Image {i}')\n",
    "            plt.axis('off')\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading image {img_paths[i]}: {e}\")\n",
    "            continue\n",
    "        \n",
    "        # Display similar images\n",
    "        for j in range(1, top_n + 1):\n",
    "            sim_idx = most_similar_pairs[i, j]\n",
    "            \n",
    "            if 0 <= sim_idx < num_images:\n",
    "                try:\n",
    "                    sim_img = plt.imread(img_paths[sim_idx])\n",
    "                    plt.subplot(1, top_n + 1, j + 1)\n",
    "                    plt.imshow(sim_img)\n",
    "                    plt.title(f'Similar {j} (Idx {sim_idx})')\n",
    "                    plt.axis('off')\n",
    "                except Exception as e:\n",
    "                    print(f\"Error loading similar image {img_paths[sim_idx]}: {e}\")\n",
    "            else:\n",
    "                break_counter += 1\n",
    "                print(f\"Warning: Index {sim_idx} is out of range for img_paths (total: {num_images}).\")\n",
    "                break\n",
    "\n",
    "        plt.show()\n",
    "\n",
    "# Call the function\n",
    "display_similar_images(img_paths, most_similar_pairs_cosine, top_n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba16d333-35f0-4b3d-b4b8-3d86cf5cd8b4",
   "metadata": {},
   "source": [
    "### * Problem 10. Explore, predict, and evaluate further\n",
    "You can do a ton of things here, at your desire. For example, how does masking different areas of the image affect classification - a method known as **saliency map** ([info](https://en.wikipedia.org/wiki/Saliency_map))? Can we detect objects? Can we significantly reduce the number of features (keeping the quality) that we get? Can we reliably train a model to predict our own classes? We'll look into these in detail in the future."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f02f1a50-ea32-41ce-9215-77a80c300dbd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
