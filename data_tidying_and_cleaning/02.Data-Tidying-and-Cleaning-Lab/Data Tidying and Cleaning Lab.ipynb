{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "078d90d4-aec6-40fe-997f-ce63c962d936",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your imports here\n",
    "\n",
    "import re\n",
    "import pycountry\n",
    "import pycountry_convert as pc\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02aa71be-5d01-4efc-85fa-a019fe92a39a",
   "metadata": {},
   "source": [
    "# Data Tidying and Cleaning Lab\n",
    "## Reading, tidying and cleaning data. Preparing data for exploration, mining, analysis and learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "259927d3-4ad5-471e-b34b-114f07127a39",
   "metadata": {},
   "source": [
    "In this lab, you'll be working with the Coffee Quality Index dataset, located [here](https://www.kaggle.com/datasets/volpatto/coffee-quality-database-from-cqi). For convenience (and to save trouble in case you can't download files, or someone uploads a newer version), I've provided the dataset in the `data/` folder. The metadata (description) is at the Kaggle link. For this lab, you'll only need `merged_data_cleaned.csv`, as it is the concatenation of the other two datasets.\n",
    "\n",
    "In this (and the following labs), you'll get several questions and problems. Do your analysis, describe it, use any tools and plots you wish, and answer. You can create any amount of cells you'd like.\n",
    "\n",
    "Sometimes, the answers will not be unique, and they will depend on how you decide to approach and solve the problem. This is usual - we're doing science after all!\n",
    "\n",
    "It's a good idea to save your clean dataset after all the work you've done to it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4450ac8e-523c-46f8-a410-9ad5af4cfc14",
   "metadata": {},
   "source": [
    "### Problem 1. Read the dataset (1 point)\n",
    "This should be self-explanatory. The first column is the index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df50bd71-ea2a-4db9-814d-c49f782ca101",
   "metadata": {},
   "outputs": [],
   "source": [
    "coffee_df = pd.read_csv('data\\.ipynb_checkpoints\\merged_data_cleaned-checkpoint.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5476b39",
   "metadata": {},
   "outputs": [],
   "source": [
    "coffee_df\n",
    "coffee_df = coffee_df.drop(columns=['Unnamed: 0'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b278e2a8-56b4-4b1a-ad71-e7b920321e37",
   "metadata": {},
   "source": [
    "### Problem 2. Observations and features (1 point)\n",
    "How many observations are there? How many features? Which features are numerical, and which are categorical?\n",
    "\n",
    "**Note:** Think about the _meaning_, not the data types. The dataset hasn't been thoroughly cleaned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7d17c46-5475-4c33-9c35-b7cf56ac41c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "coffee_df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3735ea9c",
   "metadata": {},
   "source": [
    "There are 43 features, actually 42 when we take the first column, the one holding the index, out of account, and 1339 observations. At a first glance, we can see that some of the features could potentially be messy. Such as, bag weight which needs to be int or float and is an object, harvest year, altitude, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b745e68-35eb-4acb-b39f-513137f0ee4b",
   "metadata": {},
   "source": [
    "### Problem 3. Column manipulation (1 point)\n",
    "Make the column names more Pythonic (which helps with the quality and... aesthetics). Convert column names to `snake_case`, i.e. `species`, `country_of_origin`, `ico_number`, etc. Try to not do it manually."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0a9cdab",
   "metadata": {},
   "source": [
    "To avoid manually correcting each feature name, we are going to accomplish this via a simple function which will handle the snake_case conversion for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "301e9f3f-afd6-4a91-a32f-1974584694ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_pythonic(name):\n",
    "    name = name.strip()\n",
    "    name = name.lower()\n",
    "    name = name.replace(' ', '_')\n",
    "    name = name.replace('-', '_')\n",
    "    name = name.replace('.', '_')\n",
    "    \n",
    "    return name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6455ae55",
   "metadata": {},
   "outputs": [],
   "source": [
    "coffee_df.columns = [to_pythonic(col) for col in coffee_df.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c73a644a",
   "metadata": {},
   "outputs": [],
   "source": [
    "coffee_df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c53c4c6-6eb9-4c92-bd39-89286fe4c86e",
   "metadata": {},
   "source": [
    "### Problem 4. Bag weight (1 point)\n",
    "What's up with the bag weights? Make all necessary changes to the column values. Don't forget to document your methods and assumptions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e97f3d0-1223-49a7-99a9-51a65e4ff4e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "coffee_df['bag_weight']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28393821",
   "metadata": {},
   "outputs": [],
   "source": [
    "coffee_df['bag_weight'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64a8a99f",
   "metadata": {},
   "source": [
    "Since there are only values in kg, lbs, or no unit measure, we can create a simple function to handle the conversion for us, and we will consider the values in kilograms. Let's accomplish this via a regex to filter through the different values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c77a9aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_kg(value):\n",
    "    lbs_to_kg = 0.45359237\n",
    "\n",
    "    match = re.match(r'([0-9.]+)\\s*([a-zA-Z]*)', str(value))\n",
    "    if match:\n",
    "        number = float(match.group(1))\n",
    "        unit = match.group(2).lower()\n",
    "\n",
    "        if unit in ['kg', 'kgs', 'kilogram', 'kilograms', '']:\n",
    "            return number\n",
    "        elif unit in ['lb', 'lbs', 'pound', 'pounds']:\n",
    "            return number * lbs_to_kg\n",
    "        else:\n",
    "            return None\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "coffee_df['bag_weight'] = coffee_df['bag_weight'].apply(convert_to_kg)\n",
    "\n",
    "coffee_df['bag_weight'] = coffee_df['bag_weight'].fillna(0).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "296a2d7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "coffee_df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ff1f817-8e5e-4185-a320-dde7ed2cada1",
   "metadata": {},
   "source": [
    "### Problem 5. Dates (1 point)\n",
    "This should remind you of problem 4 but it's slightly nastier. Fix the harvest years, document the process.\n",
    "\n",
    "While you're here, fix the expiration dates, and grading dates. Unlike the other column, these should be dates (`pd.to_datetime()` is your friend)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4db906d",
   "metadata": {},
   "source": [
    "Let's first check the values in this column to see what we are dealing with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "814dc5f9-9572-44f0-bff6-8efeb084f93d",
   "metadata": {},
   "outputs": [],
   "source": [
    "coffee_df['harvest_year'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f73b3743",
   "metadata": {},
   "source": [
    "The harvest_year column clearly consists of multiple variations for harvest year, only some of which are in the correct format. In order to handle this, we are going to use regex once more, where we are going to search for exact matches of the year, year with text, range of years, quarters and non-year values, and extract the year where possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67a73113",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_year(value):\n",
    "    if pd.isna(value):\n",
    "        return np.nan\n",
    "    \n",
    "    value = str(value).lower().strip()\n",
    "\n",
    "    year_match = re.match(r'^\\d{4}$', value)\n",
    "    if year_match:\n",
    "        return int(year_match.group(0))\n",
    "\n",
    "    range_match = re.match(r'(\\d{4})[^\\d]*(\\d{4})', value)\n",
    "    if range_match:\n",
    "        start_year = int(range_match.group(1))\n",
    "        end_year = int(range_match.group(2))\n",
    "        return int((start_year + end_year) / 2)\n",
    "\n",
    "    single_year_match = re.search(r'(\\d{4})', value)\n",
    "    if single_year_match:\n",
    "        return int(single_year_match.group(1))\n",
    "    \n",
    "    quarter_match = re.search(r'(\\d{1}[tq])[\\D]*(\\d{4})', value)\n",
    "    if quarter_match:\n",
    "        return int(quarter_match.group(2))\n",
    "    \n",
    "    return np.nan\n",
    "\n",
    "coffee_df['harvest_year'] = coffee_df['harvest_year'].apply(extract_year)\n",
    "\n",
    "coffee_df['harvest_year'] = coffee_df['harvest_year'].astype(pd.Int64Dtype())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52682479",
   "metadata": {},
   "outputs": [],
   "source": [
    "coffee_df['harvest_year'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ffda9bd",
   "metadata": {},
   "source": [
    "All looks good now.\n",
    "\n",
    "Let's proceed to expiration date and grading date."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a336d91f",
   "metadata": {},
   "outputs": [],
   "source": [
    "coffee_df['expiration']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29956fe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "coffee_df['grading_date']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89a1db24",
   "metadata": {},
   "source": [
    "We are going to use a similar approach to handle both columns conversion to datetime. A simple function including a regex will take care of extracting the date without ordinal suffix, then we will apply the function to both columns in order to clean the date strings throughout, and after that we will convert to datetime. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42209774",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_date(date_str, index=None):\n",
    "\n",
    "    if pd.isna(date_str) or not date_str.strip():\n",
    "        if index is not None:\n",
    "            print(f\"Index {index}: Empty or NaN value\")\n",
    "            return np.nan\n",
    "\n",
    "    date_str = re.sub(r'(\\d+)(st|nd|rd|th)', r'\\1', date_str)\n",
    "    date_str = date_str.strip()\n",
    "    \n",
    "    return date_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d780d2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Expiration date cleanup\n",
    "\n",
    "coffee_df['expiration'] = coffee_df['expiration'].apply(clean_date)\n",
    "coffee_df['expiration'] = pd.to_datetime(coffee_df['expiration'], format='%B %d, %Y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f9c5c1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grading dates cleanup\n",
    "\n",
    "coffee_df['grading_date'] = coffee_df['grading_date'].apply(clean_date)\n",
    "coffee_df['grading_date'] = pd.to_datetime(coffee_df['grading_date'], format='%B %d, %Y')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ff7a2f2",
   "metadata": {},
   "source": [
    "Both columns look good now and properly formatted."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dff33b4-c94d-43b3-bab3-97eabb862a37",
   "metadata": {},
   "source": [
    "### Problem 6. Countries (1 point)\n",
    "How many coffees are there with unknown countries of origin? What can you do about them?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0851c1a8-0420-4dba-ac27-487bae4318be",
   "metadata": {},
   "outputs": [],
   "source": [
    "unknown_values = ['', 'unknown', 'N/A', 'nan']\n",
    "coffee_df['country_of_origin'] = coffee_df['country_of_origin'].replace(unknown_values, pd.NA)\n",
    "unknown_origin_count = coffee_df['country_of_origin'].isna().sum()\n",
    "\n",
    "print(f\"Number of entries with unknown origin: {unknown_origin_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7178b76",
   "metadata": {},
   "source": [
    "There are several approaches to unknown values:\n",
    "\n",
    "1. We can simply drop them, and in this case it could be OK to do so, since it's just one entry.\n",
    "2. We can create a new category 'unknown'and keep the observations to it, which does not seem like an appropriate option in our case. \n",
    "3. We can use estimation to recover the missing values - the most frequent country of origin, other related features which can tell us what the missing value is, or a prediction model, which in this case seems like an overkill for just one missing entry.\n",
    "4. We can analyse these entries separately, in case we think they could affect the overall analysis and to observe how exactly.\n",
    "\n",
    "In our coffee case, I'd go for dropping the row completely, as it represents only a very small fraction of the whole dataset and it will unlikely affect the analysis significantly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d34494e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "coffee_df = coffee_df.dropna(subset=['country_of_origin'])\n",
    "\n",
    "unknown_origin_count = coffee_df['country_of_origin'].isna().sum()\n",
    "print(f\"Number of entries with unknown origin: {unknown_origin_count}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0aa6f30-4b93-4f23-95e0-2cafb7152c6c",
   "metadata": {},
   "source": [
    "### Problem 7. Owners (1 point)\n",
    "There are two suspicious columns, named `Owner`, and `Owner.1` (they're likely called something different after you solved problem 3). Do something about them. Is there any link to `Producer`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39bc1689-33f6-4446-bfc3-c4b4e69eccf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "coffee_df['owner']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31e4135f",
   "metadata": {},
   "outputs": [],
   "source": [
    "coffee_df['owner_1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a936168",
   "metadata": {},
   "outputs": [],
   "source": [
    "coffee_df['producer']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b773cc4e",
   "metadata": {},
   "source": [
    "What we need to do here is first to standardize owner and owner_1 columns by converting to lowercase. Then we will confirm both columns are identical, if so, we will drop owner_1, if they aren't we will merge the content of both and then drop owner_1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae0f2f56",
   "metadata": {},
   "outputs": [],
   "source": [
    "coffee_df['owner'] = coffee_df['owner'].str.lower()\n",
    "coffee_df['owner_1'] = coffee_df['owner_1'].str.lower()\n",
    "\n",
    "differences = coffee_df[coffee_df['owner'] != coffee_df['owner_1']]\n",
    "print(\"Differences between 'owner' and 'owner_1':\")\n",
    "print(differences)\n",
    "\n",
    "coffee_df['owner_combined'] = coffee_df['owner'].combine_first(coffee_df['owner_1'])\n",
    "\n",
    "print(\"Merged 'owner_combined' column:\")\n",
    "print(coffee_df[['owner', 'owner_1', 'owner_combined']])\n",
    "\n",
    "coffee_df.drop(columns=['owner', 'owner_1'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "647e1362",
   "metadata": {},
   "source": [
    "Now, let's deal with the producer column. The options are dropping NaN, filling them with default values, or \"merging\" them with data from owner_combined. In this case, since there are differences between owner and producer, although they may coincide in cases, I prefer to use a default value for producer and substitute NaN with \"unknown producer\" or something similar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46853995",
   "metadata": {},
   "outputs": [],
   "source": [
    "coffee_df['producer'] = coffee_df['producer'].fillna('Unknown Producer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93dcaeb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "coffee_df['producer']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30c53923-1b72-4500-af0a-47fdca9f57e9",
   "metadata": {},
   "source": [
    "### Problem 8. Coffee color by country and continent (1 point)\n",
    "Create a table which shows how many coffees of each color are there in every country. Leave the missing values as they are.\n",
    "\n",
    "**Note:** If you ask me, countries should be in rows, I prefer long tables much better than wide ones.\n",
    "\n",
    "Now do the same for continents. You know what continent each country is located in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72cbe6a9-ca71-4826-806d-562bc30b40d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "coffee_df['color']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a521554",
   "metadata": {},
   "source": [
    "First, we will take care of the NaN values in the color column, which we will substitute with \"unknown\" throughout, in order to have more comprehensive data better prepared for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dda6c84",
   "metadata": {},
   "outputs": [],
   "source": [
    "coffee_df['color'] = coffee_df['color'].fillna('unknown')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b73d9adc",
   "metadata": {},
   "source": [
    "We will now create pivot tables for countries and continents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84e48661",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Country-Level Pivot Table\n",
    "pivot_country = coffee_df.pivot_table(index='country_of_origin', columns='color', aggfunc='size', fill_value=0)\n",
    "pivot_country.reset_index(inplace=True)\n",
    "pivot_country_long = pivot_country.melt(id_vars='country_of_origin', var_name='color', value_name='count')\n",
    "\n",
    "print(pivot_country_long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a0a0030",
   "metadata": {},
   "outputs": [],
   "source": [
    "coffee_df['country_of_origin'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d0d217e",
   "metadata": {},
   "source": [
    "In order to create a pivot table with continent data, and not do it manually, we first need to set up a function that will utilize pycountry library, so that it can extract the continent by the given country automatically. Then, using it, we will populate our dataset with a continent column. Afterwards, we will repeat the process of creating the long pivot table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe61897b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def country_to_continent(country_name):\n",
    "    try:\n",
    "        country_alpha2 = pc.country_name_to_country_alpha2(country_name, cn_name_format=\"default\")\n",
    "        continent_code = pc.country_alpha2_to_continent_code(country_alpha2)\n",
    "        continent_name = pc.convert_continent_code_to_continent_name(continent_code)\n",
    "        return continent_name\n",
    "    except KeyError:\n",
    "        return None\n",
    "    \n",
    "coffee_df['continent'] = coffee_df['country_of_origin'].apply(country_to_continent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78b41b86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Continent-Level Pivot Table\n",
    "pivot_continent = coffee_df.pivot_table(index='continent', columns='color', aggfunc='size', fill_value=0)\n",
    "pivot_continent.reset_index(inplace=True)\n",
    "pivot_continent_long = pivot_continent.melt(id_vars='continent', var_name='color', value_name='count')\n",
    "\n",
    "print(pivot_continent_long)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27095ced-7179-4ee4-94d3-7d94450b4120",
   "metadata": {},
   "source": [
    "### Problem 9. Ratings (1 point)\n",
    "The columns `Aroma`, `Flavor`, etc., up to `Moisture` represent subjective ratings. Explore them. Show the means and range; draw histograms and / or boxplots as needed. You can even try correlations if you want. What's up with all those ratings?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d08ee355-dcbb-4657-a814-cdcfbd455c42",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist([coffee_df['aroma'], coffee_df['flavor'], coffee_df['moisture']], bins=100, label=['Aroma', 'Flavor', 'Moisture'], stacked=True)\n",
    "plt.title('Stacked Histograms of Ratings')\n",
    "plt.xlabel('Rating')\n",
    "plt.ylabel('Frequency')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80521229",
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation_af = round(coffee_df['aroma'].corr(coffee_df['flavor']), 2)\n",
    "\n",
    "print(\"Correlation between aroma and flavor:\\n\", correlation_af)\n",
    "\n",
    "correlation_mf = round(coffee_df['moisture'].corr(coffee_df['flavor']))\n",
    "print(\"Correlation between moisture and flavor:\\n\", correlation_mf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ac1ba29",
   "metadata": {},
   "source": [
    "It is clear that the moisture rating is quite low and has inverse correlation to the flavor. At the same time, aroma correlates with flavor, affecting it a great deal. Let's explore the average values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d81d23ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_flavor = coffee_df['flavor'].mean()\n",
    "avg_aroma = coffee_df['aroma'].mean()\n",
    "avg_moisture = coffee_df['moisture'].mean()\n",
    "\n",
    "print(f\"Average flavor rating: {avg_flavor:.2f}\\nAverage aroma rating: {avg_aroma:.2f}\\nAverage moisture rating: {avg_moisture:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92054218-978d-48c6-b7aa-36226837354c",
   "metadata": {},
   "source": [
    "### Problem 10. High-level errors (1 point)\n",
    "Check the countries against region names, altitudes, and companies. Are there any discrepancies (e.g. human errors, like a region not matching the country)? Take a look at the (cleaned) altitudes; there has been a lot of preprocessing done to them. Was it done correctly?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a971e5dd-4bb1-4ad6-bcbc-3cceab758f5d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3c3b5118-9f8e-45c2-a200-1be89fa4b3bf",
   "metadata": {},
   "source": [
    "### * Problem 11. Clean and explore at will\n",
    "The dataset claimed to be clean, but we were able to discover a lot of things to fix and do better.\n",
    "\n",
    "Play around with the data as much as you wish, and if you find variables to tidy up and clean - by all means, do that!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "928732cf-7ef4-471a-9818-139dd519eb45",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
