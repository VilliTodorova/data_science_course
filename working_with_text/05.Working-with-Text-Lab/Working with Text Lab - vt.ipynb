{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb8c5630-11b2-41b9-98a2-f67d6cebc323",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your imports here\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "from transformers import pipeline, DistilBertTokenizer, DistilBertForSequenceClassification\n",
    "import torch\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0375379c-19e4-4e50-9f63-5c74cf81ece5",
   "metadata": {},
   "source": [
    "# Working with Text Lab\n",
    "## Information retrieval, preprocessing, and feature extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d92edf71-988c-4e95-b17f-987095e9246a",
   "metadata": {},
   "source": [
    "In this lab, you'll be looking at and exploring European restaurant reviews. The dataset is rather tiny, but that's just because it has to run on any machine. In real life, just like with images, texts can be several terabytes long.\n",
    "\n",
    "The dataset is located [here](https://www.kaggle.com/datasets/gorororororo23/european-restaurant-reviews) and as always, it's been provided to you in the `data/` folder."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "888cf2cf-1cd7-4538-a7a2-462e80b1df99",
   "metadata": {},
   "source": [
    "### Problem 1. Read the dataset (1 point)\n",
    "Read the dataset, get acquainted with it. Ensure the data is valid before you proceed.\n",
    "\n",
    "How many observations are there? Which country is the most represented? What time range does the dataset represent?\n",
    "\n",
    "Is the sample balanced in terms of restaurants, i.e., do you have an equal number of reviews for each one? Most importantly, is the dataset balanced in terms of **sentiment**?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "596f1cf0-acfe-4e05-a953-6a2fc1ea2463",
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews = pd.read_csv('data\\European Restaurant Reviews.csv')\n",
    "\n",
    "reviews.shape\n",
    "\n",
    "reviews.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd187553",
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews['Country'].max()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46607b5a",
   "metadata": {},
   "source": [
    "Now we are going to clean up the date column in order to prepare it for datetime conversion and then we are going to extract the range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a843eb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews['Review Date'] = reviews['Review Date'].str.extract(r'([A-Za-z]+\\s+\\d{4})', expand=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a0ebc48",
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews['Review Date'] = reviews['Review Date'].str.replace('Sept', 'Sep')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5dc16d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews['Review Date'] = pd.to_datetime(reviews['Review Date'], format='%b %Y')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6837d615",
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews['Review Date'] = reviews['Review Date'].dt.to_period('M')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08199de3",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_date = reviews['Review Date'].min()\n",
    "max_date = reviews['Review Date'].max()\n",
    "\n",
    "print(f\"Date range: {min_date} to {max_date}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ad7225f",
   "metadata": {},
   "source": [
    "Next, focusing on sentiments. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42a09580",
   "metadata": {},
   "outputs": [],
   "source": [
    "restaurant_counts = reviews['Restaurant Name'].value_counts()\n",
    "\n",
    "print(restaurant_counts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81469b5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_counts = reviews['Sentiment'].value_counts()\n",
    "print(\"\\nReviews per sentiment:\\n\", sentiment_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "234b35c6",
   "metadata": {},
   "source": [
    "The dataset is unbalanced in terms of both restaurant reviews and sentiment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01a55576-cd3c-4451-b5c8-6a51356a7386",
   "metadata": {},
   "source": [
    "### Problem 2. Getting acquainted with reviews (1 point)\n",
    "Are positive comments typically shorter or longer? Try to define a good, robust metric for \"length\" of a text; it's not necessary just the character count. Can you explain your findings?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2f66395",
   "metadata": {},
   "source": [
    "We are going to use word count, sentence count and average word length. Word count will show us tendencies in review length for positive and for negative reviews. Sentence count will represent similar insight, while word length will show us the complexity of the words used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97901e74-6093-4274-966a-da73ec3a5961",
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews['Word Count'] = reviews['Review'].apply(lambda x: len(str(x).split()))\n",
    "print(reviews[['Review', 'Word Count']].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58f4b9d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_word_count_per_sentiment = reviews.groupby('Sentiment')['Word Count'].mean()\n",
    "print(avg_word_count_per_sentiment)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac7dc3ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(x='Sentiment', y='Word Count', data=reviews)\n",
    "plt.title('Distribution of Review Length by Sentiment')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d68b2d1e",
   "metadata": {},
   "source": [
    "The average word length being greater for positive reviews tells us that users perhaps tend to elaborate when giving a bad review, while they don't need to delve into specifics when giving good feedback."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66a3a74a",
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews['Sentence Count'] = reviews['Review'].apply(lambda x: len(nltk.sent_tokenize(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b483729",
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews['Average Word Length'] = reviews['Review'].apply(lambda x: np.mean([len(word) for word in re.findall(r'\\w+', x)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b457fd18",
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_sentence_count_per_sentiment = reviews.groupby('Sentiment')['Sentence Count'].mean()\n",
    "print(avg_sentence_count_per_sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f64257fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_word_length_per_sentiment = reviews.groupby('Sentiment')['Average Word Length'].mean()\n",
    "print(avg_word_length_per_sentiment)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c440fff-c079-464c-b97d-fffbf3d65baf",
   "metadata": {},
   "source": [
    "### Problem 3. Preprocess the review content (2 points)\n",
    "You'll likely need to do this while working on the problems below, but try to synthesize (and document!) your preprocessing here. Your tasks will revolve around words and their connection to sentiment. While preprocessing, keep in mind the domain (restaurant reviews) and the task (sentiment analysis)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed346623",
   "metadata": {},
   "source": [
    "Step 1: Normalization (lower case, remove punctuation, remove special characters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7114fcf9-c63a-41b2-9ff9-591a15d2ebe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    # Remove punctuation and special characters\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    return text\n",
    "\n",
    "reviews['Processed Review'] = reviews['Review'].apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "affef3b7",
   "metadata": {},
   "source": [
    "Step 2: Tokenization (break down the text into individual words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30e1fd71",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_text(text):\n",
    "    return word_tokenize(text)\n",
    "\n",
    "reviews['Tokens'] = reviews['Processed Review'].apply(tokenize_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f84ced00",
   "metadata": {},
   "source": [
    "Step 3: Removing stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc93a2ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('stopwords')\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def remove_stop_words(tokens):\n",
    "    return [word for word in tokens if word not in stop_words]\n",
    "\n",
    "reviews['Tokens Without Stop Words'] = reviews['Tokens'].apply(remove_stop_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfadaaeb",
   "metadata": {},
   "source": [
    "Step 4: Lemmatization (for the purpose of our sentiment analysis we are going to perform lemmatization because it provides more accuracy than stemming)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38202a0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('wordnet')\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def lemmatize_tokens(tokens):\n",
    "    return [lemmatizer.lemmatize(word) for word in tokens]\n",
    "\n",
    "reviews['Lemmatized Tokens'] = reviews['Tokens Without Stop Words'].apply(lemmatize_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15161333",
   "metadata": {},
   "source": [
    "Step 5: Handling rare words (remove tokens that appear less frequently)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7a5064b",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_tokens = [token for sublist in reviews['Lemmatized Tokens'] for token in sublist]\n",
    "\n",
    "token_freq = Counter(all_tokens)\n",
    "\n",
    "threshold = 5\n",
    "rare_words = set(word for word, count in token_freq.items() if count < threshold)\n",
    "\n",
    "def filter_rare_words(tokens):\n",
    "    return [word for word in tokens if word not in rare_words]\n",
    "\n",
    "reviews['Filtered Tokens'] = reviews['Lemmatized Tokens'].apply(filter_rare_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a8532af-2411-4d71-a003-0813be3f98de",
   "metadata": {},
   "source": [
    "### Problem 3. Top words (1 point)\n",
    "Use a simple word tokenization and count the top 10 words in positive reviews; then the top 10 words in negative reviews*. Once again, try to define what \"top\" words means. Describe and document your process. Explain your results.\n",
    "\n",
    "\\* Okay, you may want to see top N words (with $N \\ge 10$)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b176749",
   "metadata": {},
   "source": [
    "First, we filter reviews by sentiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faaa6dd4-251e-4e32-9137-cc6c984726be",
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_reviews = reviews[reviews['Sentiment'] == 'Positive']\n",
    "negative_reviews = reviews[reviews['Sentiment'] == 'Negative']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6f3602f",
   "metadata": {},
   "source": [
    "Second, we flatten the list of tokens and count their frequencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fb64a07",
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_tokens = [token for sublist in positive_reviews['Filtered Tokens'] for token in sublist]\n",
    "negative_tokens = [token for sublist in negative_reviews['Filtered Tokens'] for token in sublist]\n",
    "\n",
    "positive_token_freq = Counter(positive_tokens)\n",
    "negative_token_freq = Counter(negative_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "439722bb",
   "metadata": {},
   "source": [
    "Then we extract the top 10 words for both positive and negative reviews and display the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45430cbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_10_positive_words = positive_token_freq.most_common(10)\n",
    "top_10_negative_words = negative_token_freq.most_common(10)\n",
    "\n",
    "top_10_positive_df = pd.DataFrame(top_10_positive_words, columns=['Word', 'Frequency'])\n",
    "top_10_negative_df = pd.DataFrame(top_10_negative_words, columns=['Word', 'Frequency'])\n",
    "\n",
    "print(\"Top 10 Words in Positive Reviews:\")\n",
    "print(top_10_positive_df)\n",
    "print(\"\\nTop 10 Words in Negative Reviews:\")\n",
    "print(top_10_negative_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf3a16c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2, 1, figsize=(12, 10), sharex=True)\n",
    "\n",
    "# Plot top words in positive reviews\n",
    "ax[0].barh(top_10_positive_df['Word'], top_10_positive_df['Frequency'], color='skyblue')\n",
    "ax[0].set_title('Top 10 Words in Positive Reviews')\n",
    "ax[0].set_xlabel('Frequency')\n",
    "for index, value in enumerate(top_10_positive_df['Frequency']):\n",
    "    ax[0].text(value, index, f'{value}', va='center')\n",
    "\n",
    "# Plot top words in negative reviews\n",
    "ax[1].barh(top_10_negative_df['Word'], top_10_negative_df['Frequency'], color='salmon')\n",
    "ax[1].set_title('Top 10 Words in Negative Reviews')\n",
    "ax[1].set_xlabel('Frequency')\n",
    "for index, value in enumerate(top_10_negative_df['Frequency']):\n",
    "    ax[1].text(value, index, f'{value}', va='center')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4faf0ed1",
   "metadata": {},
   "source": [
    "The high frequency of the word \"would\" in the negative reviews speaks of people perhaps finding themselves more willing to visit other places after trying a given 'bad' restaurant. The appearance of \"good\" as a common word in bad reviews hints that it could have been used with negation, such as \"not very good\" or something similar.\n",
    "\n",
    "Food appearing high in frequency for both negative and positive results would mean that the people writing feedback bestow high importance on it. Similar insight can be drawn about wine and service.\n",
    "\n",
    "In positive reviews, the word \"staff\" appears, unlike in negative ones, which could hint that people tend to praise professional staff but not criticize them directly when giving bad reviews."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "790ae91f-2def-41bc-8861-49fc5467b335",
   "metadata": {},
   "source": [
    "### Problem 4. Review titles (2 point)\n",
    "How do the top words you found in the last problem correlate to the review titles? Do the top 10 words (for each sentiment) appear in the titles at all? Do reviews which contain one or more of the top words have the same words in their titles?\n",
    "\n",
    "Does the title of a comment present a good summary of its content? That is, are the titles descriptive, or are they simply meant to catch the attention of the reader?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "330b83f1-060e-47ae-8035-0b8372eed8d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_positive_words = set(word for word, _ in top_10_positive_words)\n",
    "top_negative_words = set(word for word, _ in top_10_negative_words)\n",
    "\n",
    "def contains_top_words(text, top_words):\n",
    "    return any(word in top_words for word in word_tokenize(text.lower()))\n",
    "\n",
    "reviews['Title Contains Top Positive Words'] = reviews['Review Title'].apply(lambda x: contains_top_words(x, top_positive_words))\n",
    "reviews['Title Contains Top Negative Words'] = reviews['Review Title'].apply(lambda x: contains_top_words(x, top_negative_words))\n",
    "\n",
    "positive_in_titles = reviews['Title Contains Top Positive Words'].sum()\n",
    "negative_in_titles = reviews['Title Contains Top Negative Words'].sum()\n",
    "\n",
    "print(f\"Number of titles containing top positive words: {positive_in_titles}\")\n",
    "print(f\"Number of titles containing top negative words: {negative_in_titles}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "749f2e85",
   "metadata": {},
   "outputs": [],
   "source": [
    "def title_contains_review_words(review_tokens, title_tokens):\n",
    "    return any(word in title_tokens for word in review_tokens)\n",
    "\n",
    "reviews['Title Matches Review'] = reviews.apply(\n",
    "    lambda row: title_contains_review_words(row['Filtered Tokens'], word_tokenize(row['Review Title'].lower())),\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "matching_titles_count = reviews['Title Matches Review'].sum()\n",
    "print(f\"Number of reviews where the title contains words from the review content: {matching_titles_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b91d7078",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_reviews = reviews[reviews['Title Matches Review']].sample(n=10)\n",
    "print(sample_reviews[['Review Title', 'Review']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad538630",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_distribution = reviews[reviews['Title Matches Review']]['Sentiment'].value_counts()\n",
    "print(sentiment_distribution)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a586ab6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(data=reviews[reviews['Title Matches Review']], x='Sentiment')\n",
    "plt.title('Sentiment Distribution for Reviews with Matching Titles')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc86ab92",
   "metadata": {},
   "source": [
    "According to this section of our analysis, the titles of the reviews tend to be somewhat descriptive based on the high number of matches suggesting an alignment between the most frequent words, review content and the title. The sentiment distribution plot shows a significant difference between negative and positive reviews with matching titles, which may also be due to the initial imbalance of their amounts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3a0aa94-23e4-4db1-a780-cd1bd3abb411",
   "metadata": {},
   "source": [
    "### Problem 5. Bag of words (1 point)\n",
    "Based on your findings so far, come up with a good set of settings (hyperparameters) for a bag-of-words model for review titles and contents. It's easiest to treat them separately (so, create two models); but you may also think about a unified representation. I find the simplest way of concatenating the title and content too simplistic to be useful, as it doesn't allow you to treat the title differently (e.g., by giving it more weight).\n",
    "\n",
    "The documentation for `CountVectorizer` is [here](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html). Familiarize yourself with all settings; try out different combinations and come up with a final model; or rather - two models :)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "973e2974",
   "metadata": {},
   "source": [
    "We will need separate set of hyperparameters for titles and review content. So first, we will focus on setting up and tuning a model for the titles; we will filter rare words and focus on concise but informative phrases. As for the content, there we will need more detailed analysis, we will remove rare and overly common words, as they may not be as informative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ce7892f-cb37-48d3-83fc-0ccb56c2bb1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "title_vectorizer = CountVectorizer(\n",
    "    ngram_range=(1, 2),\n",
    "    stop_words='english',\n",
    "    min_df=5,\n",
    "    max_features=1000\n",
    ")\n",
    "\n",
    "title_features = title_vectorizer.fit_transform(reviews['Review Title'])\n",
    "\n",
    "print(f\"Number of features (titles): {title_features.shape[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6978b66f",
   "metadata": {},
   "outputs": [],
   "source": [
    "content_vectorizer = CountVectorizer(\n",
    "    ngram_range=(1, 3),\n",
    "    stop_words='english',\n",
    "    min_df=5,\n",
    "    max_df=0.8,\n",
    "    max_features=5000\n",
    ")\n",
    "\n",
    "content_features = content_vectorizer.fit_transform(reviews['Review'])\n",
    "\n",
    "print(f\"Number of features (contents): {content_features.shape[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22d2b406",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "X_train_titles, X_test_titles, y_train, y_test = train_test_split(title_features, reviews['Sentiment'], test_size=0.3, random_state=42)\n",
    "X_train_contents, X_test_contents, _, _ = train_test_split(content_features, reviews['Sentiment'], test_size=0.3, random_state=42)\n",
    "\n",
    "# Train and evaluate a model for titles\n",
    "model_titles = MultinomialNB()\n",
    "model_titles.fit(X_train_titles, y_train)\n",
    "predictions_titles = model_titles.predict(X_test_titles)\n",
    "print(\"Title Model Performance:\")\n",
    "print(classification_report(y_test, predictions_titles))\n",
    "\n",
    "# Train and evaluate a model for contents\n",
    "model_contents = MultinomialNB()\n",
    "model_contents.fit(X_train_contents, y_train)\n",
    "predictions_contents = model_contents.predict(X_test_contents)\n",
    "print(\"Content Model Performance:\")\n",
    "print(classification_report(y_test, predictions_contents))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a337ce0f-2afe-422f-8e9d-beddd5635093",
   "metadata": {},
   "source": [
    "### Problem 6. Deep sentiment analysis models (1 point)\n",
    "Find a suitable model for sentiment analysis in English. Without modifying, training, or fine-tuning the model, make it predict all contents (or better, combinations of titles and contents, if you can). Measure the accuracy of the model compared to the `sentiment` column in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e579d2b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-trained sentiment analysis model and tokenizer\n",
    "model_name = 'distilbert-base-uncased-finetuned-sst-2-english'\n",
    "tokenizer = DistilBertTokenizer.from_pretrained(model_name)\n",
    "model = DistilBertForSequenceClassification.from_pretrained(model_name)\n",
    "\n",
    "sentiment_pipeline = pipeline('sentiment-analysis', model=model, tokenizer=tokenizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "622bb6fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DistilBertForSequenceClassification, DistilBertTokenizer, pipeline\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "model_name = 'distilbert-base-uncased-finetuned-sst-2-english'\n",
    "tokenizer = DistilBertTokenizer.from_pretrained(model_name)\n",
    "model = DistilBertForSequenceClassification.from_pretrained(model_name)\n",
    "\n",
    "sentiment_pipeline = pipeline('sentiment-analysis', model=model, tokenizer=tokenizer)\n",
    "\n",
    "def chunk_text(text, max_length=512):\n",
    "    tokens = tokenizer.encode(text, add_special_tokens=True)\n",
    "    \n",
    "    chunks = []\n",
    "    for i in range(0, len(tokens), max_length):\n",
    "        chunks.append(tokens[i:i + max_length])\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "def predict_sentiment(text):\n",
    "    chunks = chunk_text(text)\n",
    "\n",
    "    for i, chunk in enumerate(chunks):\n",
    "        print(f\"Chunk {i} length: {len(chunk)}\")\n",
    "    \n",
    "    sentiments = []\n",
    "    scores = []\n",
    "    \n",
    "    for chunk in chunks:\n",
    "        input_ids = torch.tensor([chunk])\n",
    "        attention_mask = torch.ones_like(input_ids)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model(input_ids, attention_mask=attention_mask)\n",
    "        logits = outputs.logits\n",
    "        predictions = torch.argmax(logits, dim=-1).item()\n",
    "        sentiment = 'POSITIVE' if predictions == 1 else 'NEGATIVE'\n",
    "        sentiments.append(sentiment)\n",
    "        scores.append(torch.nn.functional.softmax(logits, dim=-1).max().item())\n",
    "    \n",
    "    final_label = max(set(sentiments), key=sentiments.count)\n",
    "    final_score = np.mean(scores)\n",
    "    \n",
    "    return final_label, final_score\n",
    "\n",
    "def preprocess_reviews(reviews_column):\n",
    "    chunked_texts = []\n",
    "    for text in reviews_column:\n",
    "        chunks = chunk_text(text)\n",
    "        chunked_texts.append(chunks)\n",
    "    return chunked_texts\n",
    "\n",
    "def predict_sentiment_for_chunks(chunked_texts):\n",
    "    sentiments = []\n",
    "    scores = []\n",
    "    \n",
    "    for chunks in chunked_texts:\n",
    "        chunk_sentiments = []\n",
    "        chunk_scores = []\n",
    "        \n",
    "        for chunk in chunks:\n",
    "            input_ids = torch.tensor([chunk])\n",
    "            attention_mask = torch.ones_like(input_ids)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                outputs = model(input_ids, attention_mask=attention_mask)\n",
    "            logits = outputs.logits\n",
    "            predictions = torch.argmax(logits, dim=-1).item()\n",
    "            sentiment = 'POSITIVE' if predictions == 1 else 'NEGATIVE'\n",
    "            chunk_sentiments.append(sentiment)\n",
    "            chunk_scores.append(torch.nn.functional.softmax(logits, dim=-1).max().item())\n",
    "        \n",
    "        # Aggregate sentiment results for this text\n",
    "        final_label = max(set(chunk_sentiments), key=chunk_sentiments.count)  # Majority vote\n",
    "        final_score = np.mean(chunk_scores)  # Average score across chunks\n",
    "        \n",
    "        sentiments.append(final_label)\n",
    "        scores.append(final_score)\n",
    "    \n",
    "    return sentiments, scores\n",
    "\n",
    "chunked_content = preprocess_reviews(reviews['Processed Review'])\n",
    "chunked_titles = preprocess_reviews(reviews['Review Title'])\n",
    "\n",
    "reviews['Content Sentiment'], reviews['Content Sentiment Score'] = predict_sentiment_for_chunks(chunked_content)\n",
    "reviews['Title Sentiment'], reviews['Title Sentiment Score'] = predict_sentiment_for_chunks(chunked_titles)\n",
    "\n",
    "\n",
    "def combine_sentiments(title_sentiment, content_sentiment):\n",
    "    if title_sentiment == content_sentiment:\n",
    "        return title_sentiment\n",
    "\n",
    "    return content_sentiment\n",
    "\n",
    "reviews['Combined Sentiment'] = reviews.apply(\n",
    "    lambda row: combine_sentiments(row['Title Sentiment'], row['Content Sentiment']),\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "label_mapping = {'POSITIVE': 'Positive', 'NEGATIVE': 'Negative'}\n",
    "reviews['Predicted Sentiment'] = reviews['Combined Sentiment'].map(label_mapping)\n",
    "\n",
    "print(reviews[['Sentiment', 'Predicted Sentiment']])\n",
    "\n",
    "# assert 'Sentiment' in reviews.columns, \"Column 'Sentiment' not found in DataFrame\"\n",
    "\n",
    "accuracy = accuracy_score(reviews['Sentiment'], reviews['Predicted Sentiment'])\n",
    "report = classification_report(reviews['Sentiment'], reviews['Predicted Sentiment'])\n",
    "\n",
    "print(f\"Accuracy of the pre-trained sentiment analysis model: {accuracy:.2f}\")\n",
    "print(\"Classification Report:\")\n",
    "print(report)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "773d804d-9c83-48ec-be2b-8564d73d908c",
   "metadata": {},
   "source": [
    "### Problem 7. Deep features (embeddings) (1 point)\n",
    "Use the same model to perform feature extraction on the review contents (or contents + titles) instead of direct predictions. You should already be familiar how to do that from your work on images.\n",
    "\n",
    "Use the cosine similarity between texts to try to cluster them. Are there \"similar\" reviews (you'll need to find a way to measure similarity) across different restaurants? Are customers generally in agreement for the same restaurant?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f732c764-2ef0-4da2-a662-f8cd7b684245",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_embeddings(texts):\n",
    "    embeddings = []\n",
    "    for text in texts:\n",
    "        inputs = tokenizer(text, return_tensors='pt', truncation=True, padding=True)\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "\n",
    "        cls_embedding = outputs.last_hidden_state[:, 0, :].squeeze().numpy()\n",
    "        embeddings.append(cls_embedding)\n",
    "    return np.array(embeddings)\n",
    "\n",
    "def cluster_reviews(embeddings, n_clusters=5):\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=0).fit(embeddings)\n",
    "    return kmeans.labels_\n",
    "\n",
    "def analyze_clusters(reviews, cluster_labels):\n",
    "    reviews_with_clusters = reviews.copy()\n",
    "    reviews_with_clusters['Cluster'] = cluster_labels\n",
    "    return reviews_with_clusters\n",
    "\n",
    "embeddings = extract_embeddings(reviews['Processed Review'])\n",
    "\n",
    "n_clusters = 3\n",
    "cluster_labels = cluster_reviews(embeddings, n_clusters=n_clusters)\n",
    "\n",
    "clustered_reviews = analyze_clusters(reviews, cluster_labels)\n",
    "\n",
    "similarities = cosine_similarity(embeddings)\n",
    "\n",
    "print(\"Cosine Similarity Matrix:\")\n",
    "print(similarities)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07569f73",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(similarities, annot=True, cmap='coolwarm', vmin=0, vmax=1)\n",
    "plt.title('Cosine Similarity Matrix')\n",
    "plt.xlabel('Text Index')\n",
    "plt.ylabel('Text Index')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c79a585f-1dad-469a-a2ae-31d3981b9e1b",
   "metadata": {},
   "source": [
    "### \\* Problem 8. Explore and model at will\n",
    "In this lab, we focused on preprocessing and feature extraction and we didn't really have a chance to train (or compare) models. The dataset is maybe too small to be conclusive, but feel free to play around with ready-made models, and train your own."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1e90732-4d96-4692-8588-b9429430a98b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
