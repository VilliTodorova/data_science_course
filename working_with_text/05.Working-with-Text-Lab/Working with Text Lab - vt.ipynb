{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb8c5630-11b2-41b9-98a2-f67d6cebc323",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your imports here\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import re\n",
    "import nltk\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0375379c-19e4-4e50-9f63-5c74cf81ece5",
   "metadata": {},
   "source": [
    "# Working with Text Lab\n",
    "## Information retrieval, preprocessing, and feature extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d92edf71-988c-4e95-b17f-987095e9246a",
   "metadata": {},
   "source": [
    "In this lab, you'll be looking at and exploring European restaurant reviews. The dataset is rather tiny, but that's just because it has to run on any machine. In real life, just like with images, texts can be several terabytes long.\n",
    "\n",
    "The dataset is located [here](https://www.kaggle.com/datasets/gorororororo23/european-restaurant-reviews) and as always, it's been provided to you in the `data/` folder."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "888cf2cf-1cd7-4538-a7a2-462e80b1df99",
   "metadata": {},
   "source": [
    "### Problem 1. Read the dataset (1 point)\n",
    "Read the dataset, get acquainted with it. Ensure the data is valid before you proceed.\n",
    "\n",
    "How many observations are there? Which country is the most represented? What time range does the dataset represent?\n",
    "\n",
    "Is the sample balanced in terms of restaurants, i.e., do you have an equal number of reviews for each one? Most importantly, is the dataset balanced in terms of **sentiment**?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "596f1cf0-acfe-4e05-a953-6a2fc1ea2463",
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews = pd.read_csv('data\\European Restaurant Reviews.csv')\n",
    "\n",
    "reviews.shape\n",
    "\n",
    "reviews.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd187553",
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews['Country'].max()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46607b5a",
   "metadata": {},
   "source": [
    "Now we are going to clean up the date column in order to prepare it for datetime conversion and then we are going to extract the range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a843eb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews['Review Date'] = reviews['Review Date'].str.extract(r'([A-Za-z]+\\s+\\d{4})', expand=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a0ebc48",
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews['Review Date'] = reviews['Review Date'].str.replace('Sept', 'Sep')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5dc16d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews['Review Date'] = pd.to_datetime(reviews['Review Date'], format='%b %Y')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6837d615",
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews['Review Date'] = reviews['Review Date'].dt.to_period('M')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08199de3",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_date = reviews['Review Date'].min()\n",
    "max_date = reviews['Review Date'].max()\n",
    "\n",
    "print(f\"Date range: {min_date} to {max_date}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ad7225f",
   "metadata": {},
   "source": [
    "Next, focusing on sentiments. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42a09580",
   "metadata": {},
   "outputs": [],
   "source": [
    "restaurant_counts = reviews['Restaurant Name'].value_counts()\n",
    "\n",
    "print(restaurant_counts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81469b5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_counts = reviews['Sentiment'].value_counts()\n",
    "print(\"\\nReviews per sentiment:\\n\", sentiment_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "234b35c6",
   "metadata": {},
   "source": [
    "The dataset is unbalanced in terms of both restaurant reviews and sentiment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01a55576-cd3c-4451-b5c8-6a51356a7386",
   "metadata": {},
   "source": [
    "### Problem 2. Getting acquainted with reviews (1 point)\n",
    "Are positive comments typically shorter or longer? Try to define a good, robust metric for \"length\" of a text; it's not necessary just the character count. Can you explain your findings?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2f66395",
   "metadata": {},
   "source": [
    "We are going to use word count, sentence count and average word length. Word count will show us tendencies in review length for positive and for negative reviews. Sentence count will represent similar insight, while word length will show us the complexity of the words used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97901e74-6093-4274-966a-da73ec3a5961",
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews['Word Count'] = reviews['Review'].apply(lambda x: len(str(x).split()))\n",
    "print(reviews[['Review', 'Word Count']].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58f4b9d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_word_count_per_sentiment = reviews.groupby('Sentiment')['Word Count'].mean()\n",
    "print(avg_word_count_per_sentiment)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac7dc3ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(x='Sentiment', y='Word Count', data=reviews)\n",
    "plt.title('Distribution of Review Length by Sentiment')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d68b2d1e",
   "metadata": {},
   "source": [
    "The average word length being greater for positive reviews tells us that users perhaps tend to elaborate when giving a bad review, while they don't need to delve into specifics when giving good feedback."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66a3a74a",
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews['Sentence Count'] = reviews['Review'].apply(lambda x: len(nltk.sent_tokenize(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b483729",
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews['Average Word Length'] = reviews['Review'].apply(lambda x: np.mean([len(word) for word in re.findall(r'\\w+', x)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b457fd18",
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_sentence_count_per_sentiment = reviews.groupby('Sentiment')['Sentence Count'].mean()\n",
    "print(avg_sentence_count_per_sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f64257fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_word_length_per_sentiment = reviews.groupby('Sentiment')['Average Word Length'].mean()\n",
    "print(avg_word_length_per_sentiment)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c440fff-c079-464c-b97d-fffbf3d65baf",
   "metadata": {},
   "source": [
    "### Problem 3. Preprocess the review content (2 points)\n",
    "You'll likely need to do this while working on the problems below, but try to synthesize (and document!) your preprocessing here. Your tasks will revolve around words and their connection to sentiment. While preprocessing, keep in mind the domain (restaurant reviews) and the task (sentiment analysis)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7114fcf9-c63a-41b2-9ff9-591a15d2ebe5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2a8532af-2411-4d71-a003-0813be3f98de",
   "metadata": {},
   "source": [
    "### Problem 3. Top words (1 point)\n",
    "Use a simple word tokenization and count the top 10 words in positive reviews; then the top 10 words in negative reviews*. Once again, try to define what \"top\" words means. Describe and document your process. Explain your results.\n",
    "\n",
    "\\* Okay, you may want to see top N words (with $N \\ge 10$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faaa6dd4-251e-4e32-9137-cc6c984726be",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "790ae91f-2def-41bc-8861-49fc5467b335",
   "metadata": {},
   "source": [
    "### Problem 4. Review titles (2 point)\n",
    "How do the top words you found in the last problem correlate to the review titles? Do the top 10 words (for each sentiment) appear in the titles at all? Do reviews which contain one or more of the top words have the same words in their titles?\n",
    "\n",
    "Does the title of a comment present a good summary of its content? That is, are the titles descriptive, or are they simply meant to catch the attention of the reader?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "330b83f1-060e-47ae-8035-0b8372eed8d9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a3a0aa94-23e4-4db1-a780-cd1bd3abb411",
   "metadata": {},
   "source": [
    "### Problem 5. Bag of words (1 point)\n",
    "Based on your findings so far, come up with a good set of settings (hyperparameters) for a bag-of-words model for review titles and contents. It's easiest to treat them separately (so, create two models); but you may also think about a unified representation. I find the simplest way of concatenating the title and content too simplistic to be useful, as it doesn't allow you to treat the title differently (e.g., by giving it more weight).\n",
    "\n",
    "The documentation for `CountVectorizer` is [here](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html). Familiarize yourself with all settings; try out different combinations and come up with a final model; or rather - two models :)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ce7892f-cb37-48d3-83fc-0ccb56c2bb1a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a337ce0f-2afe-422f-8e9d-beddd5635093",
   "metadata": {},
   "source": [
    "### Problem 6. Deep sentiment analysis models (1 point)\n",
    "Find a suitable model for sentiment analysis in English. Without modifying, training, or fine-tuning the model, make it predict all contents (or better, combinations of titles and contents, if you can). Meaure the accuracy of the model compared to the `sentiment` column in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9292c210-ec5f-4738-8a6e-926345952848",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "773d804d-9c83-48ec-be2b-8564d73d908c",
   "metadata": {},
   "source": [
    "### Problem 7. Deep features (embeddings) (1 point)\n",
    "Use the same model to perform feature extraction on the review contents (or contents + titles) instead of direct predictions. You should already be familiar how to do that from your work on images.\n",
    "\n",
    "Use the cosine similarity between texts to try to cluster them. Are there \"similar\" reviews (you'll need to find a way to measure similarity) across different restaurants? Are customers generally in agreement for the same restaurant?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f732c764-2ef0-4da2-a662-f8cd7b684245",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c79a585f-1dad-469a-a2ae-31d3981b9e1b",
   "metadata": {},
   "source": [
    "### \\* Problem 8. Explore and model at will\n",
    "In this lab, we focused on preprocessing and feature extraction and we didn't really have a chance to train (or compare) models. The dataset is maybe too small to be conclusive, but feel free to play around with ready-made models, and train your own."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1e90732-4d96-4692-8588-b9429430a98b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
